{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a churn model using Feathr as a Feature Store\n",
    "In this sample we'll show how to use [Feathr](https://github.com/linkedin/feathr) to register the Features used to train a Machine Learning Model.\n",
    "\n",
    "We'll train a simple classification model to predict a Customers' churn using features from our Feature Store. We'll use [Azure Machine Learning](https://azure.microsoft.com/en-us/services/machine-learning/) to train our model and [Azure Databricks](https://docs.microsoft.com/en-us/azure/databricks/scenarios/what-is-azure-databricks) as the Spark Engine to process the features. \n",
    "\n",
    "**Feathr** is the feature store that is used in production in LinkedIn for many years and was open sourced in April 2022. Read our announcement on Open Sourcing Feathr and Feathr on Azure.\n",
    "\n",
    "Feathr lets you:\n",
    "\n",
    "* Define features based on raw data sources (batch and streaming) using pythonic APIs.\n",
    "* Register and get features by names during model training and model inferencing.\n",
    "* Share features across your team and company.\n",
    "\n",
    "Feathr automatically computes your feature values and joins them to your training data, using point-in-time-correct semantics to avoid data leakage, and supports materializing and deploying your features for use online in production ([source](https://github.com/linkedin/feathr#what-is-feathr)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Feathr client\n",
    "We use a `feathr_config.yaml` to define the feathr configurations. In this repo we share a sample [feathr_config file](../feathr_config.yaml.sample) so you can replace with the configs from your own environment. We also use a `config.py` file to define some sensitive data for our Service Principal credential as well as redis-password and ADLS key. You can also use an [Azure Key Vault](https://docs.microsoft.com/en-us/azure/key-vault/general/basic-concepts) to save the credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "from feathr import FeathrClient\n",
    "client = FeathrClient(config_path='../feathr_config.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your Source\n",
    "First we need to define the feature source to be used in our process. You can find more details about this process in this [Feathr doc](https://linkedin.github.io/feathr/concepts/feature-definition.html#step1-define-sources-section).\n",
    "\n",
    "We'll use a sample Dataset with calls from Customers. Below a view of this Dataset.\n",
    "\n",
    "![Calls](../Images/1-Calls.png)\n",
    "\n",
    "For each customer we have the number of calls made and the average of the call duration.\n",
    "\n",
    "**!IMPORTANT**: You have to import the example Datasets to your environment. Please import the [Dataset Folders](../Dataset/) to your dbfs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from feathr import HdfsSource\n",
    "\n",
    "batch_source = HdfsSource(name=\"Customer\",\n",
    "                          path=\"dbfs:/delta/Calls/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the [`Anchor and Features`](https://linkedin.github.io/feathr/concepts/feature-definition.html#step2-define-anchors-and-features).\n",
    "\n",
    "We'll define two features `f_NumberOfCalls` and `f_AverageCallDuration`. Here we show a very simple example only to demonstrate how we can define the features. However for more complex scenarios, involving for example transformations and calculations, you can use other Feathr routines. Take a look in the [Feathr Doc](https://linkedin.github.io/feathr/concepts/feature-definition.html#window-aggregation-features) for more use cases.\n",
    "\n",
    "We define a Feature Anchor to combine the features and sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feathr import TypedKey, ValueType, Feature, FeatureAnchor, INT32, INPUT_CONTEXT\n",
    "\n",
    "customer_id = TypedKey(key_column=\"CustomerId\",\n",
    "                       key_column_type=ValueType.INT32,\n",
    "                       description=\"CustomerId\",\n",
    "                       full_name=\"CustomerId\")\n",
    "\n",
    "features = [\n",
    "    Feature(name=\"f_NumberOfCalls\",\n",
    "            feature_type=INT32,\n",
    "            key=customer_id,\n",
    "            transform=\"NumberOfCalls\"),\n",
    "    Feature(name=\"f_AverageCallDuration\",\n",
    "            feature_type=INT32,\n",
    "            key=customer_id,\n",
    "            transform=\"AverageCallDuration\"),\n",
    "]\n",
    "\n",
    "request_anchor = FeatureAnchor(name=\"request_features\",\n",
    "                               source=batch_source,\n",
    "                               features=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Features\n",
    "Now we can build the Features indicating the anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.build_features(anchor_list=[request_anchor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the Training Data\n",
    "Now with the Features defined we can work with our [`Observation Dataset`](https://linkedin.github.io/feathr/concepts/feathr-concepts-for-beginners.html#what-are-observation-data-and-why-does-feathr-need-keys-anchor-source).\n",
    "\n",
    "In this case we need to train a model to predict the Customer's churn. So, we need to combine some Customers data with our Features using the `CustomerId` as the key to join these data. \n",
    "\n",
    "Below a sample of this Dataset:\n",
    "\n",
    "![Customer](../Images/2-Customer.png)\n",
    "\n",
    "All of the Dataset used use the Delta format. So we have to indicate in the Feathr settings the input and output format using Delta.\n",
    "\n",
    "The code below will create a Spark job on Databricks to process the Features and to combine the features with the Observation Dataset. The output could be seen in the `output_path`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feathr import FeatureQuery, ObservationSettings\n",
    "from feathr import TypedKey, ValueType, INT32\n",
    "from feathr import SparkExecutionConfiguration\n",
    "\n",
    "output_path = 'dbfs:/feathrazure_output'\n",
    "\n",
    "feature_query = FeatureQuery(\n",
    "    feature_list=[\"f_NumberOfCalls\", \"f_AverageCallDuration\"], key=customer_id)\n",
    "\n",
    "settings = ObservationSettings(\n",
    "    observation_path=\"dbfs:/delta/Customer/\")\n",
    "\n",
    "client.get_offline_features(observation_settings=settings,\n",
    "                            feature_query=feature_query,\n",
    "                            output_path=output_path,\n",
    "                            execution_configuratons=SparkExecutionConfiguration({\"spark.feathr.inputFormat\": \"delta\", \n",
    "                                                                                 \"spark.feathr.outputFormat\": \"delta\"}))\n",
    "client.wait_job_to_finish(timeout_sec=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Result\n",
    "After the Spark job succeded we can use the output to train our model. Feathr has some utils to work with the spark result and to transform it to a pandas Dataframe.\n",
    "\n",
    "![Training Data](../Images/3-Training-Data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feathr.job_utils import get_result_df\n",
    "df_res = get_result_df(client, format=\"delta\", res_url = output_path)\n",
    "\n",
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Now with the features we can train our ML Model. We can combine features from the Feature Store with other columns from the Observation Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "seed = 2022\n",
    "target_column = 'Churn'\n",
    "\n",
    "columns = ['CustomerId', 'NumberOfLoans', 'SatisfactionIndex', 'ActualBalance', 'CLTV', 'f_NumberOfCalls', 'f_AverageCallDuration', 'Churn']\n",
    "\n",
    "customer_train_dataset = df_res[columns]\n",
    "\n",
    "train, test = train_test_split(customer_train_dataset, random_state=seed, test_size=0.33)\n",
    "\n",
    "drop_columns = [target_column, 'CustomerId'] \n",
    "\n",
    "X_train = train.drop(drop_columns, axis=1)\n",
    "X_test = test.drop(drop_columns, axis=1)\n",
    "\n",
    "y_train = train[target_column]\n",
    "y_test = test[target_column]\n",
    "\n",
    "n_estimators = 3\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=n_estimators, random_state=np.random.RandomState(seed))\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materialize the Features to Online Store\n",
    "After process our feature we can materialize them to serve online models. In this case we can provide low latency and fast queries to the consumers.\n",
    "\n",
    "We'll use the [Azure Cache for Redis](https://docs.microsoft.com/en-us/azure/azure-cache-for-redis/cache-overview) an in-memory data store based on the Redis software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feathr import RedisSink, MaterializationSettings\n",
    "\n",
    "redisSink = RedisSink(table_name=\"CustomerChurn\")\n",
    "\n",
    "# Materialize two features into a redis table.\n",
    "settings = MaterializationSettings(\"CustomerChurnJob\",\n",
    "                                    sinks=[redisSink],\n",
    "                                    feature_names=[\"f_NumberOfCalls\", \"f_AverageCallDuration\"])\n",
    "\n",
    "client.materialize_features(settings, execution_configuratons=SparkExecutionConfiguration({\"spark.feathr.inputFormat\": \"delta\", \n",
    "                                                                                           \"spark.feathr.outputFormat\": \"delta\"\n",
    "                                                                                        }))\n",
    "\n",
    "res = client.get_online_features('CustomerChurn', '9996', ['f_NumberOfCalls', 'f_AverageCallDuration'])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use an Azure ML Pipeline to train, register and deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Register and Deploy the model using an Azure ML experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import ScriptRunConfig, Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "#cluster_name = '<YOUR-CLUSTER-NAME>'\n",
    "cluster_name = 'cluster-lab-01'\n",
    "compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "\n",
    "experiment_name = 'churn-experiment'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "# Define the environment\n",
    "\n",
    "# First time you run you should register the environment\n",
    "# churn_env = Environment(workspace=ws, name=\"ChurnModel-Env\")\n",
    "# conda_dep = CondaDependencies(\"../Pipelines/conda.yaml\")\n",
    "\n",
    "# Adds dependencies to PythonSection of myenv\n",
    "# churn_env.python.conda_dependencies=conda_dep\n",
    "# churn_env.register(ws)\n",
    "\n",
    "# If you already have the environment you can get it\n",
    "churn_env = Environment.get(workspace=ws, name=\"ChurnModel-Env\")\n",
    "\n",
    "# The folder with your python scripts\n",
    "project_folder = '../Pipelines'\n",
    "\n",
    "datastore_output = ws.get_default_datastore()\n",
    "\n",
    "output_data = OutputFileDatasetConfig(name=\"output_data\", \n",
    "                                      destination=(datastore_output, \"output_data/{run-id}/{output-name}\"))\n",
    "\n",
    "train_cfg = ScriptRunConfig(\n",
    "    source_directory=project_folder,\n",
    "    script=\"train.py\",\n",
    "    compute_target=compute_target,\n",
    "    environment=churn_env,\n",
    ")\n",
    "\n",
    "train_step = PythonScriptStep(\n",
    "    name=\"Train\",\n",
    "    source_directory=train_cfg.source_directory,\n",
    "    script_name=train_cfg.script,\n",
    "    runconfig=train_cfg.run_config,\n",
    "    arguments=[\"--output\", output_data]\n",
    ")\n",
    "\n",
    "deploy_cfg = ScriptRunConfig(\n",
    "    source_directory=project_folder,\n",
    "    script=\"deploy.py\",\n",
    "    compute_target=compute_target,\n",
    "    environment=churn_env    \n",
    ")\n",
    "\n",
    "deploy_step = PythonScriptStep(\n",
    "    name=\"Deploy\",\n",
    "    source_directory=train_cfg.source_directory,\n",
    "    script_name=train_cfg.script,\n",
    "    runconfig=train_cfg.run_config,\n",
    "    inputs=[output_data]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(ws, steps=[train_step, deploy_step])\n",
    "\n",
    "# Submit the run\n",
    "run = experiment.submit(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the pipeline execution we'll have the model trained and registered in the Azure Machine Learning Workspace. We'll also have an realtime endpoint deployed on AKS.\n",
    "\n",
    "![Azure ML Pipeline](../Images/4-Azure-ML-Pipeline.png)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('azureml_py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
